## CombFoldinator
##
## Snakemake pipeline for the Combfold programme. Usage is 
## pixi run make <rule name>
##

# This snakemake uses checkpoints to re-evaluate the DAG after each folding
# step. This allows the colabfold prediction to run in paralel.

import os
import json
import itertools
configfile: "config/config.yml"

# What json files are there to process in our pipeline?
INPUTS=[name.replace('.json', '') for name in os.listdir("data/") if
  name.endswith('json')]

## all -- output:
##    A file with the output of the combfold programme for each of the input
##    Files in the `data` directory.
rule all:
  input:
    expand("results/data/{protein_complex}/combfold",
        protein_complex=INPUTS)

##
## Rules
## ------
##

localrules: preprocess, run_internet_rules, setup_cuda, setup_combfold, produce_fasta_pairs, gather_pdb_pairs, produce_fasta_groups, gather_pdb_groups

## preprocess:                
##    Preprocess the input JSON files.
##
rule preprocess:
  """When the sequences are too long to predict, cut them

  Note not yet implemented yet
  """
  input:
    "data/{protein_complex}.json"
  output:
    "results/data/{protein_complex}/{protein_complex}.json"
  shell:
    """
    cp {input} {output}
    """

rule run_internet_rules:
  """This rule completes all tasks that need internet.
  """
  input:
    "results/checkpoints/setup_cuda",
    "results/checkpoints/setup_combfold"
  output:
    touch("results/checkpoints/setup_dependencies")
  shell:
    """
    echo "set all dependencies"
    """
      

## setup_combfold:              
##    Compile the COMBFOLD programme. 
##
rule setup_combfold:
  output:
    touch("results/checkpoints/setup_combfold")
  shell:
    """
    git submodule init; git submodule update
    git submodule update --remote --merge
    echo "setting up combfold"
    cd workflow/scripts/dependencies/CombFold
    echo "We are in CombFold:"
    ls
    unset PIXI_PROJECT_MANIFEST
    pixi run make
    """


## setup_cuda:                
##    Use `pip` to download cuda support programmes.
##
rule setup_cuda:
  conda:
    "envs/fold.yml"
  output:
    touch("results/checkpoints/setup_cuda")
  script:
    "scripts/dependencies/setup-cuda.sh"

## produce_fasta_pair: 
##    Convert the json's to fasta inputs.
##
checkpoint produce_fasta_pairs:
  """Make the fasta for larger subunit pairs
  """
  input:
    "results/data/{protein_complex}/{protein_complex}.json",
    "results/checkpoints/setup_combfold"
  output:
    directory("results/data/{protein_complex}/subunits/fasta-pairs/")
  shell:
    """
    cd workflow/scripts/dependencies/CombFold
    unset PIXI_PROJECT_MANIFEST
    pixi run python3 scripts/prepare_fastas.py ../../../../{input[0]} \
      --stage pairs --output-fasta-folder ../../../../{output[0]} \
      --max-af-size 1800
    """

# Function to get the list of produced FASTA files -- CHATGPT
def get_fasta_files(wildcards):
    checkpoint_output = checkpoints.produce_fasta_pairs.get(**wildcards).output[0]
    fasta_files = [os.path.join(checkpoint_output, f) for f in
      os.listdir(checkpoint_output) if f.endswith(".fasta")]
    return fasta_files


# Function to get the list of produced pdb files -- CHATGPT
def get_pdb_files_pairs(wildcards):
    checkpoint_output = checkpoints.produce_fasta_pairs.get(**wildcards).output[0]
    fasta_files = [os.path.join(checkpoint_output, f) for f in
      os.listdir(checkpoint_output) if f.endswith(".fasta")]
    pdb_file = [(file
      .replace("fasta-pairs", "processed-pairs")
      .replace(".fasta", "")) 
      for file in fasta_files]
    return pdb_file
    
def get_pdb_files_groups(wildcards):
    checkpoint_output = checkpoints.produce_fasta_groups.get(**wildcards).output[0]
    fasta_files = [os.path.join(checkpoint_output, f) for f in
      os.listdir(checkpoint_output) if f.endswith(".fasta")]
    pdb_file = [(file
      .replace("fasta-groups", "processed-groups")
      .replace(".fasta", "")) 
      for file in fasta_files]
    return pdb_file


## colabfold:
##    Calculate the protein structures of the fasta files using Colabfold.
##
rule colabfold:
  conda:
    "envs/fold.yml"
  input:
    fasta="results/data/{protein_complex}/subunits/fasta-{grouping}/{fasta_file}.fasta",
    cuda="results/checkpoints/setup_cuda"
  resources:
    mem_mb=32000,
    slurm_extra= "'--gres=gpu:a100:1'",
    constraint="gpu",
    tasks="1",
    cpus_per_task=10,
    mem="30G",
    slurm_partition='gpu1',
    runtime='24h',
    slurm_account="mpmp_gpu",
  # threads: 10
  params:
    number_of_models=config['colabfold']['number_of_models']
  output:
    directory("results/data/{protein_complex}/subunits/processed-{grouping}/{fasta_file}")
  shell:
    """
    echo "Starting colabfold, please see {output}/log.txt for the log"
    colabfold_batch {input.fasta} {output} --num-models {params.number_of_models}
    """

rule gather_pdb_pairs:
  input:
    get_pdb_files_pairs
  output:
    directory("results/data/{protein_complex}/subunits/pair-pdb/")
  shell:
    """
    mkdir {output} -p
    echo "Copying PDB files to: {output}"
    for pdb_dir in {input}; do
      echo "Processing directory: $pdb_dir"
      echo "In this directory are the following files:"
      ls $pdb_dir -l
      cp "$pdb_dir"/*.pdb {output}
    done
    """

## produce_fasta_groups:
##    Determine which higher order-groups are most usefull to predict models
##    for.
##
checkpoint produce_fasta_groups:
  input:
    "results/data/{protein_complex}/{protein_complex}.json",
    "results/data/{protein_complex}/subunits/pair-pdb",
    "results/checkpoints/setup_combfold"
  output:
    directory("results/data/{protein_complex}/subunits/fasta-groups/")
  shell:
    """
    cd workflow/scripts/dependencies/CombFold
    unset PIXI_PROJECT_MANIFEST
    pixi run python3 scripts/prepare_fastas.py ../../../../{input[0]} \
      --stage groups --output-fasta-folder ../../../../{output[0]} \
      --max-af-size 1800 \
      --input-pairs-results ../../../../{input[1]}
    """

rule gather_pdb_groups:
  input:
    get_pdb_files_groups
  output:
    directory("results/data/{protein_complex}/subunits/group-pdb/")
  shell:
    """
    mkdir {output} -p
    echo "Copying PDB files to: {output}"
    for pdb_dir in {input}; do
      echo "Processing directory: $pdb_dir"
      echo "In this directory are the following files:"
      ls $pdb_dir -l
      cp "$pdb_dir"/*.pdb {output}
    done
    """

rule gather_pdb:
  input:
    pairs="results/data/{protein_complex}/subunits/pair-pdb",
    groups="results/data/{protein_complex}/subunits/group-pdb"
  output: 
    directory("results/data/{protein_complex}/subunits/pdb")
  shell:
    """
    echo "Collating the structures..."
    ls {input.pairs} -l
    ls {input.groups} -l
    mkdir {output} -p
    cp {input.pairs}/*pdb {output}
    cp {input.groups}/*pdb {output}
    """


## combfold:                  
##    Calculate the protein complexes from the (higher order) pairs
##    predicted by the colabfold steps.
##
rule combfold:
  """Run the combfold programme

  Note --- If a high scoring assembly cannot be found, the programme will
            exit with a nonzero exit code. That is why the usage of 
            `set +e` is needed.
  """
  input:
    "results/checkpoints/setup_combfold",
    json="results/data/{protein_complex}/{protein_complex}.json",
    pdb="results/data/{protein_complex}/subunits/pdb"
  output:
    directory("results/data/{protein_complex}/combfold")
  resources:
    mem_mb=12000
  threads: 15
  shell:
    """
    set +e
    cd workflow/scripts/dependencies/CombFold
    unset PIXI_PROJECT_MANIFEST
    pixi run python3 scripts/run_on_pdbs.py ../../../../{input.json}  \
      ../../../../{input.pdb} output-{wildcards.protein_complex}
    mkdir ../../../../{output} -p
    mv output-{wildcards.protein_complex}/* ../../../../{output}
    rm output-{wildcards.protein_complex} -r
    """

## help:
##    Show the help.
##
rule help:
  input: "workflow/Snakefile"
  shell:
      "sed -n 's/^##//p' {input}"

## version:
##    Show the version.
##
rule version:
  shell:
    """
    git describe --tags --abbrev=0
    """
    

## clean:                     
##    Clean all outputs of the data folder.
##
rule clean:
    shell:
        "rm -rf results/data*"

## build_overview:            
##    Print the graph of the current job.
##
rule build_overview:
  conda:
    "envs/utils.yml"
  output:
    "results/method.{fileformat}"
  shell:
    """
    snakemake -c 1 --forceall --dag | dot -T{wildcards.fileformat} > {output}
    """

rule install_easy_graph:
  conda:
    "envs/utils.yml"
  output:
    touch("results/checkpoints/install_easy_graph")
  shell:
    """
    cpan -i App::cpanminus
    cpanm Graph::Easy
    """

## build_overview_ascii:
##    Prints the graph in ascii format.
rule build_ascii_graph:
  conda:
    "envs/utils.yml"
  input:
    "results/checkpoints/install_easy_graph"
  output:
    "results/method-simple.ascii"
  shell:
    """
    snakemake -c 1 --forceall --dag > out
    graph-easy --from=dot --as_ascii out >  {output}
    rm out
    """
